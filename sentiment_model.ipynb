{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Justification for Model\n",
    "\n",
    "I decided to use a tfidf vectorizer to transform the text into a vector.  This is generally better than a simple count vectorizer because it takes into account the fact that words that don't show up often in general but do show up in a particular document or more predictive than common words that just show up all the time.  \n",
    "\n",
    "I decided to use a linear support vector classifer because the number of features (the length of the tfidf vectorizer) is relatively large compared to our number of samples so it will be useful to have a maximum margin classifier that can still converge even in the case of linearly separable data as overfitting is definitely a problem that we will run into in this regime.\n",
    "\n",
    "In order to deal with overfitting I made both the support vector classifier's regularization and the length of the tfidf vectors hyperparameters of the model and used a scikit-learn pipeline to put these together and allow us to use cross-validation to tune both of these hyperparameters together instead of separately since they both control overfitting and will both have some influence on each other.\n",
    "\n",
    "In order to validate the model I used the f1 score instead of simply using the accuracy because the dataset is imbalanced with 10 times as many positives as negatives.  As a result a model that predicts a positive every time is already 91% accurate making it not a great metric to use.  The f1 scores takes into account both the precision and the recall which would prevent a model that always predicts a positive from having a good score.\n",
    "\n",
    "# Model Results\n",
    "\n",
    "Surprisingly it turns out that the best result is a region of the hyperparameter space where there is clear overfitting.  The model performs almost perfectly on the training data (misclassifying only 3 out of 33000) with both f1 and accuracy scores above .999. It still performs well on the test set of data that it has not been trained on with f1 and accuracy scores of .789 and .965.  This is a clear sign of overfitting but nevertheless this parameter configuration leads to our best f1 score on the test set despite the overfitting.  This suggests that the model does not have enough data to train on and given more data it could probably perform significantly better so given the opportunity to collect more data that would probably be one of the most important things that could be done to improve this model.\n",
    "\n",
    "# Other things to try\n",
    "\n",
    "There are a lot of other things that I would have liked to try but I only had a few hours to devote to this today so I will instead just write a little bit about the things I would have tried if I had more time and why.\n",
    "\n",
    "Looking at the confusion matrix we can see that it is doing a much better job predicting the positives than the negatives.  One interesting way to try to deal with this could be using a model that outputs a probability such as logistic regression and making the cutoff probability for a positive/negative in the model a hyperparameter to try to deal with this issue and see if there is some optimal cutoff other than 50%.  Though what is an optimal cutoff could depend on business considerations.  For example in some applications a false positive is worse than a false negative or vice versa.\n",
    "\n",
    "Another thing that I would have really loved to try if I had more time is some neural network models and in particular I would have liked to try to make use of word2vec.  There is a lot of information that my tfidf/svm method is not making use of.  First of all tfidf does not take into account the order of the word or the context of the words in the reviews.  It would be possible to use n-grams instead of only using single words to create our tfidf vectors but we are already overfitting so that probably wouldn't be that useful.  Second of all the relationship of different words to each other is also not taken into account by tfidf whereas a model making use of something like word2vec would be able to make use of the fact that words such as fantastic and excellent are synonyms and treat them as such when making predictions.  The only issue with neural networks is that 33000 samples may not be enough to train it but I think it is certainly worth a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great fun!, Got these last Christmas as a gag ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inspiring, I hope a lot of people hear this cd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great CD, My lovely Pat has one of the GREAT v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First album I've bought since Napster, We've c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazing!, I used to find myself starting Chron...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  sentiment\n",
       "0  Great fun!, Got these last Christmas as a gag ...          1\n",
       "1  Inspiring, I hope a lot of people hear this cd...          1\n",
       "2  Great CD, My lovely Pat has one of the GREAT v...          1\n",
       "3  First album I've bought since Napster, We've c...          1\n",
       "4  Amazing!, I used to find myself starting Chron...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30000\n",
       "1     3000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data set is unbalanced\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use random_state to make results reproduceable\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['sentiment'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_features': (500, 1000, 2000, 5000, 10000, 20000, 50000),\n",
    "    'clf__C': (0.1, 0.5, 1.0, 5.0, 10.0)\n",
    "}\n",
    "\n",
    "# use pipeline to be able to tune both svm regularization and TfidfVectorizer max_features at the same time\n",
    "# both are parameters that will be useful to control overfitting\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(max_df=0.5,\n",
    "                             min_df=2, stop_words='english',\n",
    "                             use_idf=True)),\n",
    "    ('clf', svm.LinearSVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# do cross validation grid search automatically using scikit learn\n",
    "grid_search = GridSearchCV(pipeline, parameters, scoring='f1', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 47s, sys: 2.45 s, total: 3min 50s\n",
      "Wall time: 3min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'vect__max_features': (500, 1000, 2000, 5000, 10000, 20000, 50000), 'clf__C': (0.1, 0.5, 1.0, 5.0, 10.0)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 5.0, 'vect__max_features': 20000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.96250828,  0.92858839,  0.91574804,  0.92127554,  0.93918562,\n",
       "         0.92526627,  0.92912046,  0.91089026,  0.90428019,  0.91202235,\n",
       "         0.91720581,  0.93000563,  0.93461585,  0.93688019,  0.91953365,\n",
       "         0.92298277,  0.91576266,  0.93210371,  0.95045733,  0.9573427 ,\n",
       "         0.93409133,  1.00227102,  0.97752778,  0.97100242,  0.96816897,\n",
       "         0.97880975,  0.99074705,  0.99329201,  1.08759022,  1.04261335,\n",
       "         1.03319224,  1.01983897,  1.01538698,  1.02401408,  1.02455012]),\n",
       " 'mean_score_time': array([ 0.41898068,  0.39545043,  0.39674807,  0.40232205,  0.41273069,\n",
       "         0.4092234 ,  0.41359234,  0.39749368,  0.39924002,  0.39909093,\n",
       "         0.40417886,  0.41439033,  0.41269207,  0.40839044,  0.39023868,\n",
       "         0.39494626,  0.39901622,  0.41023517,  0.40772899,  0.4165717 ,\n",
       "         0.41165082,  0.39084832,  0.39181153,  0.404459  ,  0.40322971,\n",
       "         0.40716259,  0.42241565,  0.41252438,  0.39203715,  0.3953704 ,\n",
       "         0.40001814,  0.40375543,  0.40659642,  0.40974625,  0.40797027]),\n",
       " 'mean_test_score': array([ 0.55102154,  0.62401735,  0.63652815,  0.63512675,  0.62134785,\n",
       "         0.60652215,  0.60652215,  0.60921655,  0.69238749,  0.72661514,\n",
       "         0.74892835,  0.75346345,  0.75387443,  0.75387443,  0.61886898,\n",
       "         0.69755219,  0.73030809,  0.75942499,  0.77193962,  0.77464186,\n",
       "         0.77464186,  0.6296432 ,  0.69108991,  0.69683679,  0.74200878,\n",
       "         0.76975613,  0.77709055,  0.77709055,  0.63258575,  0.68711278,\n",
       "         0.68071662,  0.73003484,  0.75949957,  0.77497859,  0.77497859]),\n",
       " 'mean_train_score': array([ 0.57510189,  0.6661252 ,  0.70652349,  0.7215117 ,  0.7146647 ,\n",
       "         0.70192072,  0.70192072,  0.65806699,  0.7657642 ,  0.84801989,\n",
       "         0.91195456,  0.94007056,  0.95261101,  0.95261101,  0.66895433,\n",
       "         0.78491017,  0.88121038,  0.95496832,  0.97864771,  0.98608966,\n",
       "         0.98608966,  0.68072223,  0.80568698,  0.93585001,  0.99647163,\n",
       "         0.99937837,  0.99989632,  0.99989632,  0.68253759,  0.80898501,\n",
       "         0.95501148,  0.99937804,  1.        ,  1.        ,  1.        ]),\n",
       " 'param_clf__C': masked_array(data = [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1.0 1.0 1.0 1.0\n",
       "  1.0 1.0 1.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 10.0 10.0 10.0 10.0 10.0 10.0 10.0],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__max_features': masked_array(data = [500 1000 2000 5000 10000 20000 50000 500 1000 2000 5000 10000 20000 50000\n",
       "  500 1000 2000 5000 10000 20000 50000 500 1000 2000 5000 10000 20000 50000\n",
       "  500 1000 2000 5000 10000 20000 50000],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__C': 0.1, 'vect__max_features': 500},\n",
       "  {'clf__C': 0.1, 'vect__max_features': 1000},\n",
       "  {'clf__C': 0.1, 'vect__max_features': 2000},\n",
       "  {'clf__C': 0.1, 'vect__max_features': 5000},\n",
       "  {'clf__C': 0.1, 'vect__max_features': 10000},\n",
       "  {'clf__C': 0.1, 'vect__max_features': 20000},\n",
       "  {'clf__C': 0.1, 'vect__max_features': 50000},\n",
       "  {'clf__C': 0.5, 'vect__max_features': 500},\n",
       "  {'clf__C': 0.5, 'vect__max_features': 1000},\n",
       "  {'clf__C': 0.5, 'vect__max_features': 2000},\n",
       "  {'clf__C': 0.5, 'vect__max_features': 5000},\n",
       "  {'clf__C': 0.5, 'vect__max_features': 10000},\n",
       "  {'clf__C': 0.5, 'vect__max_features': 20000},\n",
       "  {'clf__C': 0.5, 'vect__max_features': 50000},\n",
       "  {'clf__C': 1.0, 'vect__max_features': 500},\n",
       "  {'clf__C': 1.0, 'vect__max_features': 1000},\n",
       "  {'clf__C': 1.0, 'vect__max_features': 2000},\n",
       "  {'clf__C': 1.0, 'vect__max_features': 5000},\n",
       "  {'clf__C': 1.0, 'vect__max_features': 10000},\n",
       "  {'clf__C': 1.0, 'vect__max_features': 20000},\n",
       "  {'clf__C': 1.0, 'vect__max_features': 50000},\n",
       "  {'clf__C': 5.0, 'vect__max_features': 500},\n",
       "  {'clf__C': 5.0, 'vect__max_features': 1000},\n",
       "  {'clf__C': 5.0, 'vect__max_features': 2000},\n",
       "  {'clf__C': 5.0, 'vect__max_features': 5000},\n",
       "  {'clf__C': 5.0, 'vect__max_features': 10000},\n",
       "  {'clf__C': 5.0, 'vect__max_features': 20000},\n",
       "  {'clf__C': 5.0, 'vect__max_features': 50000},\n",
       "  {'clf__C': 10.0, 'vect__max_features': 500},\n",
       "  {'clf__C': 10.0, 'vect__max_features': 1000},\n",
       "  {'clf__C': 10.0, 'vect__max_features': 2000},\n",
       "  {'clf__C': 10.0, 'vect__max_features': 5000},\n",
       "  {'clf__C': 10.0, 'vect__max_features': 10000},\n",
       "  {'clf__C': 10.0, 'vect__max_features': 20000},\n",
       "  {'clf__C': 10.0, 'vect__max_features': 50000}],\n",
       " 'rank_test_score': array([35, 29, 25, 26, 30, 33, 33, 32, 21, 18, 14, 13, 11, 11, 31, 19, 16,\n",
       "        10,  7,  5,  5, 28, 22, 20, 15,  8,  1,  1, 27, 23, 24, 17,  9,  3,\n",
       "         3], dtype=int32),\n",
       " 'split0_test_score': array([ 0.53040541,  0.61750405,  0.6272578 ,  0.62270451,  0.61031276,\n",
       "         0.58914729,  0.58914729,  0.58487654,  0.67445255,  0.72396213,\n",
       "         0.74672489,  0.75761974,  0.75826598,  0.75826598,  0.60105184,\n",
       "         0.67811159,  0.72739917,  0.75886032,  0.77591036,  0.7740113 ,\n",
       "         0.7740113 ,  0.61323529,  0.67571235,  0.69345431,  0.73967213,\n",
       "         0.772     ,  0.77358491,  0.77358491,  0.61549708,  0.66532529,\n",
       "         0.68047709,  0.72200772,  0.75921053,  0.77272727,  0.77272727]),\n",
       " 'split0_train_score': array([ 0.58040513,  0.66376238,  0.71010535,  0.72308892,  0.7191188 ,\n",
       "         0.70621019,  0.70621019,  0.66059917,  0.77037562,  0.84993179,\n",
       "         0.91023936,  0.93760209,  0.95118008,  0.95118008,  0.67337058,\n",
       "         0.78958333,  0.88352085,  0.95155598,  0.97627333,  0.98396731,\n",
       "         0.98396731,  0.68342441,  0.81443995,  0.92802057,  0.99595142,\n",
       "         0.99968915,  1.        ,  1.        ,  0.68600435,  0.81550802,\n",
       "         0.94730159,  0.99968896,  1.        ,  1.        ,  1.        ]),\n",
       " 'split1_test_score': array([ 0.55455291,  0.6211878 ,  0.63004847,  0.62303231,  0.6156434 ,\n",
       "         0.60085106,  0.60085106,  0.61550038,  0.69884726,  0.72039943,\n",
       "         0.73890909,  0.74285714,  0.74573758,  0.74573758,  0.61697417,\n",
       "         0.70522648,  0.7231405 ,  0.75924634,  0.76034237,  0.76224784,\n",
       "         0.76224784,  0.62922966,  0.69261745,  0.68737864,  0.74161736,\n",
       "         0.76566757,  0.77472527,  0.77472527,  0.6323319 ,  0.68787276,\n",
       "         0.67044025,  0.73337637,  0.75555556,  0.77823129,  0.77823129]),\n",
       " 'split1_train_score': array([ 0.57892545,  0.67532468,  0.71500975,  0.72862163,  0.71721959,\n",
       "         0.70348139,  0.70348139,  0.66515837,  0.76768402,  0.85281981,\n",
       "         0.91308691,  0.93948315,  0.95136187,  0.95136187,  0.67627494,\n",
       "         0.78840782,  0.88077696,  0.95222724,  0.97916667,  0.98681733,\n",
       "         0.98681733,  0.68770403,  0.80908783,  0.93910256,  0.99719713,\n",
       "         0.99968896,  1.        ,  1.        ,  0.6884058 ,  0.81506389,\n",
       "         0.95919013,  0.99968896,  1.        ,  1.        ,  1.        ]),\n",
       " 'split2_test_score': array([ 0.56810631,  0.63336019,  0.65227818,  0.65964344,  0.63808739,\n",
       "         0.62956811,  0.62956811,  0.62727273,  0.70386266,  0.73548387,\n",
       "         0.76115108,  0.75991348,  0.75761974,  0.75761974,  0.63858093,\n",
       "         0.7093185 ,  0.74038462,  0.7601683 ,  0.77956613,  0.78766643,\n",
       "         0.78766643,  0.64646465,  0.70493992,  0.70967742,  0.74473684,\n",
       "         0.7716008 ,  0.78296146,  0.78296146,  0.64992826,  0.7081403 ,\n",
       "         0.69123253,  0.73472042,  0.76373263,  0.7739772 ,  0.7739772 ]),\n",
       " 'split2_train_score': array([ 0.5659751 ,  0.65928854,  0.69445537,  0.71282455,  0.70765569,\n",
       "         0.69607057,  0.69607057,  0.64844343,  0.75923295,  0.84130809,\n",
       "         0.91253741,  0.94312642,  0.95529109,  0.95529109,  0.65721746,\n",
       "         0.77673936,  0.87933333,  0.96112173,  0.98050314,  0.98748436,\n",
       "         0.98748436,  0.67103825,  0.79353318,  0.94042689,  0.99626633,\n",
       "         0.99875699,  0.99968896,  0.99968896,  0.67320261,  0.79638312,\n",
       "         0.95854271,  0.99875622,  1.        ,  1.        ,  1.        ]),\n",
       " 'std_fit_time': array([ 0.03697081,  0.02465337,  0.01077862,  0.00653976,  0.00670742,\n",
       "         0.00113414,  0.00751335,  0.00285432,  0.01020205,  0.00205324,\n",
       "         0.00572554,  0.00187478,  0.00879431,  0.0071489 ,  0.00187492,\n",
       "         0.01671672,  0.01046509,  0.01406348,  0.00309235,  0.00858998,\n",
       "         0.00538687,  0.00943487,  0.00329233,  0.00690905,  0.00546776,\n",
       "         0.00818117,  0.01111601,  0.00441057,  0.00130782,  0.01096488,\n",
       "         0.02455533,  0.0136228 ,  0.01562412,  0.00933245,  0.00686717]),\n",
       " 'std_score_time': array([ 0.02575695,  0.00120695,  0.00030415,  0.00144278,  0.00405191,\n",
       "         0.00412612,  0.01007998,  0.00162856,  0.00373147,  0.00244446,\n",
       "         0.0058879 ,  0.0088399 ,  0.00542285,  0.00329518,  0.00032028,\n",
       "         0.00543006,  0.00237774,  0.00590356,  0.00575954,  0.00669867,\n",
       "         0.00135296,  0.00378152,  0.00193159,  0.00399635,  0.00414516,\n",
       "         0.003442  ,  0.00589311,  0.00215547,  0.0044404 ,  0.00430422,\n",
       "         0.004782  ,  0.00739193,  0.00375858,  0.00315321,  0.00504276]),\n",
       " 'std_test_score': array([ 0.01559257,  0.0067754 ,  0.01119507,  0.01733643,  0.01203503,\n",
       "         0.01698198,  0.01698198,  0.01786941,  0.01284614,  0.0064376 ,\n",
       "         0.00921296,  0.00755803,  0.00575967,  0.00575967,  0.01537966,\n",
       "         0.01384772,  0.00733423,  0.00054872,  0.0083352 ,  0.01038667,\n",
       "         0.01038667,  0.01356898,  0.0119809 ,  0.00941239,  0.0020861 ,\n",
       "         0.00289563,  0.00417739,  0.00417739,  0.01405762,  0.01748742,\n",
       "         0.0084901 ,  0.00570249,  0.00334453,  0.00235594,  0.00235594]),\n",
       " 'std_train_score': array([ 0.00648182,  0.00675656,  0.00876519,  0.00654486,  0.0050164 ,\n",
       "         0.00428406,  0.00428406,  0.00705484,  0.00474721,  0.00489022,\n",
       "         0.0012334 ,  0.00229323,  0.00189655,  0.00189655,  0.00838349,\n",
       "         0.00579753,  0.0017368 ,  0.00435974,  0.00176537,  0.00152524,\n",
       "         0.00152524,  0.00706698,  0.00886745,  0.00556259,  0.00052887,\n",
       "         0.00043938,  0.00014663,  0.00014663,  0.00667323,  0.00891273,\n",
       "         0.00545812,  0.0004397 ,  0.        ,  0.        ,  0.        ])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred_train = grid_search.predict(X_train)\n",
    "y_pred_test = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99937823834196893"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78899082568807344"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99988636363636363"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9651515151515152"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23986,     1],\n",
       "       [    2,  2411]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5940,  158],\n",
       "       [  72,  430]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can see that the positives are being predicted much better than the negatives\n",
    "confusion_matrix(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
